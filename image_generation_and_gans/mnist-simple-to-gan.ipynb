{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bobfShNWaBRl"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3AHbGeBaEwx"
      },
      "source": [
        "# MNIST Experimentation\n",
        "\n",
        "In this file I will experiment with creating a simple image classification network, and then develop it into a simple GAN, from scratch as much as possible. For this, we will be using the MNIST dataset, a dataset of 28x28 pixel images containing handwritten numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrqYfVWTwBfK"
      },
      "source": [
        "## Part I: Data acquisition and EDA\n",
        "\n",
        "First things first we will download the data and have a look through it to observe the properties of the data we are dealing with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUSfQCGIa-KW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "        content = requests.get(URL + FILENAME).content\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J0qYM70bbFv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import tqdm\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCzP7OCXbB79"
      },
      "outputs": [],
      "source": [
        "print(f\"Training input shape: {x_train.shape}\")\n",
        "print(f\"Training output shape: {y_train.shape}\")\n",
        "print(f\"Validation input shape: {x_valid.shape}\")\n",
        "print(f\"Validation output shape: {y_valid.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x9FeRCSipMK"
      },
      "outputs": [],
      "source": [
        "# lets look at our labels\n",
        "\n",
        "print(f\"Label set: {set(y_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyy-Qzxgb253"
      },
      "outputs": [],
      "source": [
        "example_image = x_train[0]\n",
        "print(f\"Example image raw: \\n{example_image}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk6hFjjkcLrR"
      },
      "source": [
        "Obviously this is a little hard to parse, but the important detail is that this is a sparsely populated vector, with values $0 \\geq x \\geq 1$ . What this points to is a greyscale image where 0 represents white space and 1 represents black, and gradiations in between represent corresponding shades of grey."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oBZ-MXqdWK8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "\n",
        "example_image_reshaped = example_image.reshape((28, 28))\n",
        "print(f\"Example image reshaped: \\n{example_image_reshaped}\")\n",
        "\n",
        "pyplot.imshow(example_image_reshaped, cmap=\"gray\")\n",
        "example_label = y_train[0]\n",
        "print(f\"Example label: {example_label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FO5fwZscy_V"
      },
      "source": [
        "Makes sense. So now our tensor is shaped (28, 28), how should we interpret this. The first dimension can be thought of as each pixels row, and the second dimension can be thought of as the column. To see how, lets artificially crop the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejL-sc36b6aj"
      },
      "outputs": [],
      "source": [
        "cropped_along_horizontal_axis = example_image_reshaped[:len(example_image_reshaped)//2]\n",
        "pyplot.imshow(cropped_along_horizontal_axis, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4BhXg2RhVDa"
      },
      "outputs": [],
      "source": [
        "cropped_along_vertical_axis = example_image_reshaped[:, :len(example_image_reshaped)//2]\n",
        "pyplot.imshow(cropped_along_vertical_axis, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNcz0EkLjBo7"
      },
      "source": [
        "## Part II: First Pass Convolutional Model\n",
        "\n",
        "First we are going to step by step make a convolutional model which categorises the images.\n",
        "\n",
        "From some online research I found a \"general\" setup:\n",
        "\n",
        "> We will stack 3 {convolution + relu + maxpooling} modules. Our convolutions operate on 3x3 windows and our maxpooling layers operate on 2x2 windows. Our first convolution extracts 16 filters, the following one extracts 32 filters, and the last one extracts 64 filters.\n",
        "\n",
        "That comes from [this tutorial](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/pc/exercises/image_classification_part1.ipynb?utm_source=practicum-IC&utm_campaign=colab-external&utm_medium=referral&hl=en&utm_content=imageexercise1-colab#scrollTo=5oqBkNBJmtUv). First, lets do some package imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjLmgv5AXyL7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import utils as vutils\n",
        "\n",
        "# set random seed\n",
        "\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKFgwKs8ZQvw"
      },
      "source": [
        "### The convolution layers.\n",
        "\n",
        "First, lets see how the convolution layers work. Lets take the reshaped example image and pass it through some example convolutional layers to inspect their behavior.\n",
        "\n",
        "Convolutional layers expect tensors of the form $\n",
        " (C, H, W) $ where H and W are the height and width respectively in pixels of the image, and C are the channels of the image. Most images have a channel of 3, representing the exact colour in the pixel via a RGB (red-green-blue) coordinate. However, our image is grayscale, and therefore one-dimensional. Therefore, we simply need to unsqueeze the original 28x28 images to have an extra 1-dimension to the tensor to fit the input specifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2sieRz9ZPGt"
      },
      "outputs": [],
      "source": [
        "example_image_tensor = torch.tensor(example_image_reshaped, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "conv_layer_1_filter = nn.Conv2d(1, 1, 3) # the arguments here are (channel, num_filters, convolution_pixel_size)\n",
        "output_1_filter = conv_layer_1_filter(example_image_tensor)\n",
        "\n",
        "conv_layer_16_filter = nn.Conv2d(1, 16, 3) # the arguments here are (channel, num_filters, convolution_pixel_size)\n",
        "output_16_filter = conv_layer_16_filter(example_image_tensor)\n",
        "\n",
        "print(output_1_filter.shape)\n",
        "print(output_16_filter.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M0K2wLndpvu"
      },
      "source": [
        "Lets just quickly see what this would look like if we did have multiple channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA2KCG1sdwS5"
      },
      "outputs": [],
      "source": [
        "example_image_tensor_3_channels = example_image_tensor.repeat(3, 1, 1)\n",
        "\n",
        "print(f\"New tensor shape: {example_image_tensor_3_channels.shape}\")\n",
        "\n",
        "conv_layer_1_filter_3_channels = nn.Conv2d(3, 1, 3)\n",
        "\n",
        "output_1_filter_3_channels = conv_layer_1_filter_3_channels(example_image_tensor_3_channels)\n",
        "\n",
        "print(f\"Output shape: {output_1_filter_3_channels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME4QJVoXfO4G"
      },
      "source": [
        "Next, lets observe how max pooling works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y-wDzFRfTXB"
      },
      "outputs": [],
      "source": [
        "max_pool_layer = nn.MaxPool2d(2)\n",
        "\n",
        "output_max_pool = max_pool_layer(output_16_filter)\n",
        "\n",
        "print(output_max_pool.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPEkIx4rfaF1"
      },
      "source": [
        "We've halved the height and width of the filters by taking only the most significant values for each 2x2 square in each filter. Lets take an example, max pool it, and observe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r3qHO7IfuEk"
      },
      "outputs": [],
      "source": [
        "example = torch.rand(1, 26, 26)\n",
        "max_pooled_example = max_pool_layer(example)\n",
        "print(example[0, :2])\n",
        "print(max_pooled_example[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448OOH8ghf_b"
      },
      "source": [
        "It's a little hard to read, but you can see by taking the first two elements of the first and second row in the example, the highest of those will be represented in the max-pooled output. Lets spell it out a bit more explicitly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djDBaMvvXWzr"
      },
      "outputs": [],
      "source": [
        "def explicit_max_pool(i):\n",
        "  kernel = example[0][0, i:i+2].tolist() + example[0][1, i:i+2].tolist()\n",
        "  print(f\"The current kernel captures values: {kernel}. The max pooled value is {max(kernel)}\")\n",
        "\n",
        "explicit_max_pool(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvGB-mIJXXHt"
      },
      "source": [
        "Now we understand these processes, lets make our network matching the specifications laid out above. Note, by the way, the flatten process simply stretches out the dimensions of the tensor into one long array, while ReLU converts all values in the tensor below 0 to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8PhGKKUXYgr"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalNetworkV1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sequence = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, 3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(16, 32, 3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(32, 64, 3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(64, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.sequence(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usFL2gykidNj"
      },
      "outputs": [],
      "source": [
        "conv_net_v1 = ConvolutionalNetworkV1()\n",
        "output_v1 = conv_net_v1(example_image_tensor.unsqueeze(0))\n",
        "output_v1.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfSZMBPfjqlH"
      },
      "source": [
        "The model is outputting data in the correct format! Hooray! Lets send our model to the GPU and build a training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8_CAqXEQT-V"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  conv_net_v1.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFaxJjz2i0X9"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(conv_net_v1.parameters(), lr=0.001)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "x_valid_tensor = torch.tensor(x_valid, dtype=torch.float32)\n",
        "y_valid_tensor = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
        "valid_dataset = torch.utils.data.TensorDataset(x_valid_tensor, y_valid_tensor)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XaFo4UTkVz-"
      },
      "outputs": [],
      "source": [
        "def calculate_validation_loss_and_f1(model, valid_dataloader):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  predicted_labels = []\n",
        "  true_labels = []\n",
        "  for batch in valid_dataloader:\n",
        "    x, y = batch\n",
        "    x_reshaped = x.unsqueeze(1).reshape(-1, 1, 28, 28).to(\"cuda\")\n",
        "    y = y.to(\"cuda\")\n",
        "    output = model(x_reshaped)\n",
        "    loss = loss_function(output, y)\n",
        "    total_loss += loss.item()\n",
        "  return total_loss / len(valid_dataloader), multiclass_f1_score(output, y, num_classes=10, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVB2_KOAkpb9"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "show_loss = False\n",
        "train = False\n",
        "\n",
        "if train:\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    conv_net_v1.train()\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      x, y = batch\n",
        "      optim.zero_grad()\n",
        "      x_reshaped = x.unsqueeze(1).reshape(-1, 1, 28, 28).to(\"cuda\")\n",
        "      y = y.to(\"cuda\")\n",
        "      output = conv_net_v1(x_reshaped)\n",
        "      loss = loss_function(output, y)\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "      if show_loss and i % 100 == 0:\n",
        "        print(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "    validation_loss, f1 = calculate_validation_loss_and_f1(conv_net_v1, valid_dataloader)\n",
        "    print(f\"Epoch: {epoch}, Validation Loss: {validation_loss}, F1: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWm5WqoVR4PI"
      },
      "outputs": [],
      "source": [
        "conv_net_v1.eval().to(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QgZmSLDk8A5"
      },
      "outputs": [],
      "source": [
        "conv_net_v1(example_image_tensor.unsqueeze(0).reshape(-1, 1, 28, 28)).max(1).indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtCmfJdaoyoB"
      },
      "outputs": [],
      "source": [
        "pyplot.imshow(example_image_reshaped, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RyeTGK1pMF1"
      },
      "source": [
        "It worked! Lets try another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkdqSDa5o_W_"
      },
      "outputs": [],
      "source": [
        "example_2 = x_train[1]\n",
        "example_2_reshaped = example_2.reshape((28, 28))\n",
        "example_2_tensor = torch.tensor(example_2_reshaped, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "conv_net_v1(example_2_tensor.unsqueeze(0).reshape(-1, 1, 28, 28)).max(1).indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1VBcgoCpi39"
      },
      "outputs": [],
      "source": [
        "pyplot.imshow(example_2_reshaped, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzhNJLSXpmoG"
      },
      "source": [
        "Another success! Time to move on.\n",
        "\n",
        "## Part III: Developing a GAN\n",
        "\n",
        "Generative Adversarial Networks are composed of a generator network and a discriminator network. Discriminator networks have the responsibility of determining true examples of a category from false examples of a category. We already have a discriminator whose current responsibility is classifying the images into each digit.\n",
        "\n",
        "The generator is a model which takes in random noise and generates an output which should look like the training images, at least enough to fool the discriminator. For our purposes, lets just make this a standard feedforward neural network.\n",
        "\n",
        "First, lets modify our convolutional network to make it into a discriminator which classifies inputs images as 1 (an image from the training set) or 0 (an image created by the generator)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMGZQEMqppG7"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sequence = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, 3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(16, 32, 3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(32, 64, 3),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(64, 1) # THIS IS THE ONLY CHANGED LINE FROM BEFORE\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.sequence(x)\n",
        "\n",
        "discriminator = Discriminator()\n",
        "discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFu8aNYwZuKp"
      },
      "source": [
        "Now let's create a super simple generator. This will be a small feedforward network with a Sigmoid layer at the end to convert the output into a 28x28 pixel image with all values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8LMaOj9vwy3"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sequence = nn.Sequential(\n",
        "        nn.Linear(100, 100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100, 100),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(100, 28*28),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.sequence(x)\n",
        "\n",
        "generator = Generator()\n",
        "generator_optim = torch.optim.Adam(generator.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdzyaMALwiTs"
      },
      "source": [
        "Sigmoid functions convert all elements to be between 0 and 1, see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjKjs-QSv9qH"
      },
      "outputs": [],
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "elements = torch.tensor([-5, -2, 0, 2, 5])\n",
        "sigmoid(elements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK2D29-GbnqH"
      },
      "source": [
        "We also need to be able to create our dataset. For GANs, the only data we really care about keeping on hand is 'true' training data. 'False' training data can be generated by sending random noise through the generator. So, lets make a dataset for the 'true' data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmY9PfAj0trX"
      },
      "outputs": [],
      "source": [
        "def create_true_image_dataloader(device):\n",
        "  true_images = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
        "  dataset = torch.utils.data.TensorDataset(true_images)\n",
        "  return torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWd1Yi19hg_W"
      },
      "source": [
        "Now we can create our annotated dataset, we need a loss function to train the discriminator. This can just be BCE loss.\n",
        "\n",
        "We also need to store a true label and a false label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N77miZU7hfP2"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "real_label = 1.0\n",
        "fake_label = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxsYBNdHhojg"
      },
      "source": [
        "We now define a function which generates random noise for our generator to use to produce an image. Our generator is a simple feedforward network, so the noise is just to be the size of our input feature vector (100), by a batch size. This can then be passed to our generator's forward method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1qhBkHh4tFA"
      },
      "outputs": [],
      "source": [
        "def get_noise(batch_size):\n",
        "  return torch.randn(\n",
        "        batch_size, 100,\n",
        "        device=device\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66_9I2hzn4rz"
      },
      "source": [
        "Now we write the important learning methods. We alternate between training the generator and the discriminator. The important thing here is that we are using the binary cross entropy loss to backpropagate loss through **both** the discriminator and the generator to alternatingly train both.\n",
        "\n",
        "The discriminator is fed in both real and fake images as 1 channel, 28x28 vectors. Its task is to correctly classify the real and fake images. Cross entropy loss is a function which can represent this problem. So, when we feed in our data, we get a loss that penalises the model the more it incorrectly categorises the images it sees. This loss gets backpropagated through the discriminator and we use the optimiser to update the **weights of the discriminator only.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2QQzx333C6k"
      },
      "outputs": [],
      "source": [
        "def train_discriminator(\n",
        "    batch,\n",
        "    discriminator,\n",
        "    generator,\n",
        "    discriminator_optim,\n",
        "    ):\n",
        "  \n",
        "  # Step 1: Train the discriminator on the real images\n",
        "  discriminator_optim.zero_grad()\n",
        "  output = discriminator(batch).view(-1)\n",
        "  label = torch.full((batch.shape[0],), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "  t_loss = loss_function(output, label)\n",
        "  t_loss.backward()\n",
        "  D_x = output.mean().item() # the average prediction of the discriminator for real images\n",
        "\n",
        "\n",
        "  # Step 2: Train the discriminator on the fake images\n",
        "  noise = get_noise(batch.shape[0])\n",
        "  generated_images = generator(noise)\n",
        "  generated_images = generated_images.reshape(-1, 1, 28, 28)\n",
        "  output = discriminator(generated_images).view(-1)\n",
        "  label.fill_(fake_label)\n",
        "\n",
        "  f_loss = loss_function(output, label)\n",
        "  f_loss.backward()\n",
        "  D_G_z1 = output.mean().item() # the average prediction of the discriminator for fake images\n",
        "\n",
        "  loss = t_loss + f_loss # total loss for the batch\n",
        "  discriminator_optim.step()\n",
        "\n",
        "  return loss.item(), D_G_z1, D_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvlj7NXQpZf4"
      },
      "source": [
        "The generator's update function performs an inversion of the discriminators loss. We similarly use BCE loss, however we are only concerned with the fake images. The loss here penalises the discriminator for **not classifying the images as true.** This loss then gets backpropagated through the discriminator and into the generator. The optimiser then **solely updates the parameters of the generator**.\n",
        "\n",
        "*NB: this technique can sometimes be vulnerable to the vanishing gradient problem, where the discriminator is extremely confident in classifying the image and does not give a good signal for which way the generator can improve to produce more convincing images.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur7rZc0V4ZlN"
      },
      "outputs": [],
      "source": [
        "def train_generator(\n",
        "    batch,\n",
        "    discriminator,\n",
        "    generator,\n",
        "    generator_optim,\n",
        "    ):\n",
        "  generator_optim.zero_grad()\n",
        "\n",
        "  noise = get_noise(batch.shape[0])\n",
        "  generated_images = generator(noise)\n",
        "  generated_images = generated_images.reshape(-1, 1, 28, 28)\n",
        "  output = discriminator(generated_images)\n",
        "  loss = loss_function(output.squeeze(), torch.ones(batch.shape[0], dtype=torch.float32, device=device))\n",
        "\n",
        "  loss.backward()\n",
        "  generator_optim.step()\n",
        "  D_G_z2 = output.mean().item()\n",
        "\n",
        "  return loss.item(), D_G_z2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OXbfRwIJJDV"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "fixed_noise = get_noise(64)\n",
        "fixed_noise = fixed_noise.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhhtmUnEffq9"
      },
      "outputs": [],
      "source": [
        "\n",
        "generated_image_vectors = []\n",
        "\n",
        "def train_epoch(epoch, discriminator, generator, discriminator_optim, generator_optim, generated_image_vectors):\n",
        "  dataloader = create_true_image_dataloader(device)\n",
        "  discriminator.train().to(device)\n",
        "  generator.train().to(device)\n",
        "\n",
        "  epoch_d_loss = 0\n",
        "  epoch_g_loss = 0\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for batch in tqdm.tqdm(dataloader):\n",
        "\n",
        "    batch_true_images = batch[0].reshape(-1, 1, 28, 28)\n",
        "    # train discriminator\n",
        "    d_loss, D_G_z1, D_x = train_discriminator(\n",
        "        batch_true_images,\n",
        "        discriminator,\n",
        "        generator,\n",
        "        discriminator_optim,\n",
        "        )\n",
        "    # train generator\n",
        "    g_loss, D_G_z2 = train_generator(\n",
        "        batch_true_images,\n",
        "        discriminator,\n",
        "        generator,\n",
        "        generator_optim,\n",
        "        )\n",
        "\n",
        "    epoch_d_loss += d_loss\n",
        "    epoch_g_loss += g_loss\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  generator.eval() # shouldn't make any difference here, but best practice\n",
        "  with torch.no_grad():\n",
        "    fake = generator(fixed_noise).detach().cpu().reshape(-1, 1, 28, 28)\n",
        "\n",
        "  generated_image_vectors.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "\n",
        "  print(f\"Epoch {epoch} discriminator loss: {epoch_d_loss / len(dataloader)}\")\n",
        "  print(f\"Epoch {epoch} generator loss: {epoch_g_loss / len(dataloader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yTt4Xpu23N2"
      },
      "outputs": [],
      "source": [
        "for epoch in range(10):\n",
        "  train_epoch(\n",
        "      epoch,\n",
        "      generator=generator,\n",
        "      generator_optim=generator_optim,\n",
        "      discriminator=discriminator,\n",
        "      discriminator_optim=discriminator_optim,\n",
        "      generated_image_vectors=generated_image_vectors\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC39qDyR6H4f"
      },
      "outputs": [],
      "source": [
        "pyplot.imshow(np.transpose(generated_image_vectors[-1],(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlwutlsv_0XT"
      },
      "source": [
        "Our model is learning! The issue, however, is that our generator's output sucks. Lets make a new generator, following the inverse of our discriminator network. This follows a [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) on GANs released on PyTorch's website.\n",
        "\n",
        "The core idea here is that where we use Conv2d layers in the discriminator, we use ConvTranspose2d here. ConvTranspose2d are 'transposed convolution operations'. This sounds confusing and scary. What you should understand it as is the inverse of a convolution operation. You know how convolution takes lots of filters and uses them on an image to create smaller versions of that image? This should be imagined as taking small versions of images and exploding them out into larger images with fewer features. From end-to-end, we can transform a n-dimensional array (conceptually a 1-pixel image with n features), into our 28x28 pixel image with a feature depth of 1 (whether the pixel is black or white).\n",
        "\n",
        "We implement that below, where our input 100-dimensional feature array gets transformed into a 28x28 image with 1 feature, and finally applying a sigmoid function to convert the outputs to be between 0 and 1 (as our greyscale image values are required to be)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR79l_M17dii"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 128, 4, stride=1, padding=0),  # Output: (batch, 128, 4, 4)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),   # Output: (batch, 64, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),    # Output: (batch, 32, 16, 16)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),     # Output: (batch, 1, 32, 32)\n",
        "            # note we now use a convolutional layer, not a transpose convolutional layer to get to the final image\n",
        "            nn.Conv2d(1, 1, kernel_size=5),                        # Output: (batch, 1, 28, 28)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-mBIoW3tSXE"
      },
      "source": [
        "The consequence to this new generator architecture is that its inputs are expected to take a different shape, that being $ (B, C, H, W)$, respectively batch, channel (the feature dimension), height, and width. So lets alter our noise function that creates input for our generator, and train again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY88KvYJiPbL"
      },
      "outputs": [],
      "source": [
        "def get_noise(batch_size):\n",
        "  return torch.randn(\n",
        "        batch_size, 100, 1, 1,\n",
        "        device=device\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNx8OCZRCXiI"
      },
      "outputs": [],
      "source": [
        "fixed_noise = get_noise(64)\n",
        "fixed_noise = fixed_noise.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktfGJkUpm0HL"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "generator_optim = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "discriminator = Discriminator()\n",
        "discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "generated_image_vectors = []\n",
        "\n",
        "for epoch in range(10):\n",
        "  train_epoch(\n",
        "      epoch,\n",
        "      generator=generator,\n",
        "      generator_optim=generator_optim,\n",
        "      discriminator=discriminator,\n",
        "      discriminator_optim=discriminator_optim,\n",
        "      generated_image_vectors=generated_image_vectors,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BN-NQtZm5_B"
      },
      "outputs": [],
      "source": [
        "pyplot.imshow(np.transpose(generated_image_vectors[-1],(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMXXSMK2aoqr"
      },
      "source": [
        "The results you'll see from here could be extremely varied. Sometimes the results can look a bit like hand-drawn numbers, and other times its complete garbage. Lets try some alterations, again following [this tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html).\n",
        "\n",
        "The first this I'll do is to give the generator more layers, and more of a chance to learn. When training, I'll lower the learning rate for the discriminator and raise it for the generator. The hypothesis is that because the discriminator is outperforming the generator, we'll give the generator more of an advantage. The potential downside is that the learning rate will prevent the generator finding a true optimum as it bounces around too wildly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szSIaOfHdg4D"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(50, 128, 4, 1, 0, bias=False),  # Output: (batch, 256, 4, 4)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),  # Output: (batch, 128, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),   # Output: (batch, 64, 16, 16)\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias=False),   # Output: (batch, 64, 16, 16)\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Conv2d(16, 1, kernel_size=5),   # Output: (batch, 1, 28, 28)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "Generator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x93XNRnTEdd9"
      },
      "outputs": [],
      "source": [
        "def get_noise(batch_size):\n",
        "  return torch.randn(\n",
        "        batch_size, 50, 1, 1,\n",
        "        device=device\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_cOCN3VVFlx"
      },
      "outputs": [],
      "source": [
        "fixed_noise = get_noise(64)\n",
        "fixed_noise = fixed_noise.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNzOv-IFDOy2"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "generator_optim = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "discriminator = Discriminator()\n",
        "discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "generated_image_vectors = []\n",
        "\n",
        "for epoch in range(10):\n",
        "  train_epoch(\n",
        "      epoch,\n",
        "      generator=generator,\n",
        "      generator_optim=generator_optim,\n",
        "      discriminator=discriminator,\n",
        "      discriminator_optim=discriminator_optim,\n",
        "      generated_image_vectors=generated_image_vectors,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7EjT_ywERJ9"
      },
      "outputs": [],
      "source": [
        "pyplot.imshow(np.transpose(generated_image_vectors[-8],(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii4iBcNsFGj0"
      },
      "source": [
        "This doesn't seem to result in a huge change. Lets try something else. One technique commonly used in these models is LeakyReLU. This is a variation on ReLU.\n",
        "\n",
        "ReLU usually simply sets all negative activations in a layer to 0. LeakyReLU allows negative signals to \"leak through\" by providing a small slope at negative values rather than just setting them to 0. This gives more information to propagate in the network. Given we're propagating information back through the discriminator into the generator, we're giving the generator **more information** from the discriminator for it to use to improve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErGRw8dNFCQO"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is ``(nc) x 64 x 64``\n",
        "            nn.Conv2d(1, 16, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf) x 32 x 32``\n",
        "            nn.Conv2d(16, 32, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*2) x 16 x 16``\n",
        "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*4) x 8 x 8``\n",
        "            nn.Conv2d(64, 1, 4, 2, 1, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2dOZlKqFdsZ"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "generator_optim = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "discriminator = Discriminator()\n",
        "discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "generated_image_vectors = []\n",
        "\n",
        "for epoch in range(10):\n",
        "  train_epoch(\n",
        "      epoch,\n",
        "      generator=generator,\n",
        "      generator_optim=generator_optim,\n",
        "      discriminator=discriminator,\n",
        "      discriminator_optim=discriminator_optim,\n",
        "      generated_image_vectors=generated_image_vectors,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta6qox0JFqxl"
      },
      "outputs": [],
      "source": [
        "pyplot.imshow(np.transpose(generated_image_vectors[-1],(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N8jD-AnGORi"
      },
      "source": [
        "We're still not seeing a large change! Lets add beta parameters for Adam optimizers and modify the learning rate as in the tutorial.\n",
        "\n",
        "(Go into what Betas do)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQH-Sde7GiJe"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "generator_optim = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "discriminator = Discriminator()\n",
        "discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "generated_image_vectors = []\n",
        "\n",
        "for epoch in range(10):\n",
        "  train_epoch(\n",
        "      epoch,\n",
        "      generator=generator,\n",
        "      generator_optim=generator_optim,\n",
        "      discriminator=discriminator,\n",
        "      discriminator_optim=discriminator_optim,\n",
        "      generated_image_vectors=generated_image_vectors,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYG5Sb52Wzaw"
      },
      "outputs": [],
      "source": [
        "pyplot.imshow(np.transpose(generated_image_vectors[-1],(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvmEruZQHdqm"
      },
      "source": [
        "We (finally) see a big improvement. We went from very junky output to something that basically fits the bill of hand-drawn digits. It does still look quite artificial (a human could probably tell most of these are generated). Lets try one more trick implemented in the tutorial, custom weight initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U4gar42GVwd"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH5MxcdkHjBx"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "generator.apply(weights_init)\n",
        "generator_optim = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "discriminator = Discriminator()\n",
        "discriminator.apply(weights_init)\n",
        "discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "generated_image_vectors = []\n",
        "\n",
        "for epoch in range(10):\n",
        "  train_epoch(\n",
        "      epoch,\n",
        "      generator=generator,\n",
        "      generator_optim=generator_optim,\n",
        "      discriminator=discriminator,\n",
        "      discriminator_optim=discriminator_optim,\n",
        "      generated_image_vectors=generated_image_vectors,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3YTSaZBeKzF"
      },
      "outputs": [],
      "source": [
        "pyplot.imshow(np.transpose(generated_image_vectors[-1],(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TePFg30KIajT"
      },
      "source": [
        "This definitely results in some more improvement. The numbers look a lot more natural. By far, however, we can see the biggest improvement came from setting the learning rate and beta parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moOVHmUqyUTr"
      },
      "source": [
        "## Closing throughts\n",
        "\n",
        "We've now created a functional GAN network that can draw digits pretty convincingly. We have learned about:\n",
        "\n",
        "- image data\n",
        "- convolutional operations, both normal and transposed\n",
        "- GAN architectures and learning algorithms\n",
        "- improvements on GANs that take their output from garbage to realistic images\n",
        "\n",
        "Something you can use this notebook in particular to play around with is the order in which you apply the improvements I've talked about in this notebook. What I noticed, for example, was that setting the right learning rate and beta value were much more important than modifying the architecture of the model too much. In fact, in rough order of importance I would rank the improvements in the following way:\n",
        "\n",
        "1. Beta and LR values\n",
        "2. Weight initialization\n",
        "3. Discriminator leaky ReLU\n",
        "4. Generator additional layers\n",
        "\n",
        "You might find different results! I hope this tutorial was helpful üòÅ thank you for following along!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
