{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "e-W_CHdBe82T",
    "outputId": "a790d3ac-672e-4af8-9604-37a5654246a0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwTJ9mS5e82U"
   },
   "source": [
    "## Part 1: Data\n",
    "\n",
    "We iterate through the Netflix dataset text files and prepare the matrix M, a matrix with columns of movies and users. If a user has watched a movie, the index corresponding to the (movie, user) pair will be set to 1, else 0.\n",
    "\n",
    "### Data prep\n",
    "\n",
    "We do a simple regex to get the movie patterns and then find all the users that have watched that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWl2KHtMe82V"
   },
   "outputs": [],
   "source": [
    "movie_expression = re.compile(r\"(\\d+):\")\n",
    "\n",
    "def parse(lines: list):\n",
    "    movie_id = None\n",
    "    movies_and_users = []\n",
    "    for line in tqdm.tqdm(lines):\n",
    "        is_movie = movie_expression.search(line)\n",
    "        if is_movie:\n",
    "            movie_id = is_movie.groups()[0]\n",
    "            continue\n",
    "        user_id, _, _ = line.split(',')\n",
    "        movies_and_users.append((int(movie_id), int(user_id)))\n",
    "    return movies_and_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byE-so_ee82V",
    "outputId": "fad7273a-77bd-46b7-8195-cd1bcf4a3792"
   },
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"dataset/combined_data_3.txt\", # uncomment the below line to process the full dataset, but be warned it's huge.\n",
    "#     \"/kaggle/input/netflix-prize-data/combined_data_4.txt\", \"/kaggle/input/netflix-prize-data/combined_data_1.txt\", \"/kaggle/input/netflix-prize-data/combined_data_2.txt\"\n",
    "]\n",
    "movies_and_users = []\n",
    "for f in files:\n",
    "    print(f'processing file {f}')\n",
    "    with open(f, \"r\") as raw_text:\n",
    "        lines = raw_text.readlines()\n",
    "    movies_and_users.extend(parse(lines[:260578])) # even just one file is BIG so I've just been taking a subset of ~250,000 lines, remove the slice to process the full file\n",
    "    print(f'completed processing file {f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cflIMjOae82V",
    "outputId": "5a49ca34-1f59-4a12-96a6-a20498a27f39"
   },
   "outputs": [],
   "source": [
    "movies, users = zip(*movies_and_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw5nAqM8e82W",
    "outputId": "1b92ab82-ea6e-42a7-a2ae-da40f0f6c1f3"
   },
   "outputs": [],
   "source": [
    "print(movies_and_users[:10])\n",
    "print(movies[:5])\n",
    "print(users[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have movie/user pairs. However, we need to turn the IDs into indexes. Lets do that below to convert this data into something we can index into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkjDWcoRe82W"
   },
   "outputs": [],
   "source": [
    "unique_movies = sorted(list(set(movies)))\n",
    "unique_users = sorted(list(set(users)))\n",
    "\n",
    "movie_to_idx = {movie: i for i, movie in enumerate(unique_movies)}\n",
    "user_to_idx = {user: i for i, user in enumerate(unique_users)}\n",
    "\n",
    "normalised_movies = [movie_to_idx[m] for m in movies]\n",
    "normalised_users = [user_to_idx[u] for u in users]\n",
    "\n",
    "normalised_movies_and_users = list(zip(normalised_movies, normalised_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the matrix M representing the co-occurrence of a movie and a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9NYgHLDe82W",
    "outputId": "664daa05-8165-49e6-8653-e8ed8e82d1a2"
   },
   "outputs": [],
   "source": [
    "M = torch.zeros(len(unique_movies), len(unique_users))\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets populate the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7XcYN7oe82W"
   },
   "outputs": [],
   "source": [
    "for movie, user in normalised_movies_and_users:\n",
    "    M[movie, user] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zqhwngye82X"
   },
   "source": [
    "# Recommendation\n",
    "\n",
    "We have generated a matrix M of all films and all viewers, with the value of each index *i,j* corresponding to whether a given user *i* watched film *j*. This matrix is factorised into two smaller matrices U, F which represent embeddings for users and films respectively. We can iteratively hold each matrix constant while we tune the other to better match the results in M.\n",
    "\n",
    "First, lets create our smaller matrices U and F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cwbwky6Te82X"
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 200\n",
    "\n",
    "U = torch.randn(len(unique_users), embedding_dimension, requires_grad=True, device=device)\n",
    "F = torch.randn(len(unique_movies), embedding_dimension, requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_umWgQeoe82X",
    "outputId": "06234a55-a0bd-4196-815b-d1c24564cd32"
   },
   "outputs": [],
   "source": [
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our two matrices now exist to embed films and movies. Furthermore, when we transpose one matrix and multiply with the other, we get a matrix with shape equal to size M, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_hat = (F @ U.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to create a loss function between our created matrix M_hat and true matrix M, then backpropagate to the embedding networks.\n",
    "\n",
    "We follow the Google tutorial [here](https://developers.google.com/machine-learning/recommendation/collaborative/matrix) and perform Weighted Matrix Factorization.\n",
    "\n",
    "The 'weighted' element here comes from the loss function. Because our co-occurrences are distributed sparsely, our model has the option early to learn most effectively by simply classifying every (movie, user) pair as 0. So, we weight the loss from co-occurrences much more highly than the non-co-occurences.\n",
    "\n",
    "The way we do this is by first masking out the non-occurrences and finding the difference between M and M_hat. This provides the loss for the co-occurrences. Next we do the inverse, masking out the co-occurrences to get the loss for the non-occurrences. Then we add the losses together in a weighted fashion. Simple, right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uh2P5klee82X"
   },
   "outputs": [],
   "source": [
    "M = M.to(device)\n",
    "optim = torch.optim.Adam([U, F], 0.01)\n",
    "# optim = torch.optim.AdamW([U, F], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K8IS0WTe82X"
   },
   "outputs": [],
   "source": [
    "def training_loop(display):\n",
    "    M_hat = F @ U.T\n",
    "    difference = M - M_hat\n",
    "    loss_1 = (difference * M) ** 2\n",
    "    non_observation_mask = 1 - M\n",
    "    loss_2 = (difference * non_observation_mask) ** 2\n",
    "    loss = loss_1 + (0.005 * loss_2)\n",
    "    loss = loss.mean()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if display:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHLIVQMxe82X",
    "outputId": "989b9458-f443-4925-81cb-5c7053970af9"
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    display = True if i % 100 == 0 else False\n",
    "    training_loop(display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is decreasing, which means M_hat is increasingly similar to M. Lets check that out directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLtVV7Ele82X",
    "outputId": "264489e9-ef72-4f2b-83e1-3f78170ab1b2"
   },
   "outputs": [],
   "source": [
    "M[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5kG-dBNe82Y",
    "outputId": "5b98e890-1f07-49d4-ad6c-2a983885ae61"
   },
   "outputs": [],
   "source": [
    "(F @ U.T)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, our positive co-occurrences are near 1, and our negative occurrences nearby 0. We did it! One additional potential improvement we could add is an element-wise sigmoid function to our M_hat to get all our outputs near 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "def training_loop(display):\n",
    "    M_hat = sigmoid(F @ U.T)\n",
    "    difference = M - M_hat\n",
    "    loss_1 = (difference * M) ** 2\n",
    "    non_observation_mask = 1 - M\n",
    "    loss_2 = (difference * non_observation_mask) ** 2\n",
    "    loss = loss_1 + (0.005 * loss_2)\n",
    "    loss = loss.mean()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if display:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    display = True if i % 100 == 0 else False\n",
    "    training_loop(display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid((F @ U.T)[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this doesn't actually provide us much of a benefit. In fact, it does a little worse than where we were before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The choice of optimizer happens to be very important. Try training again but substituting AdamW for the much more simple optimizer, SGD. You'll notice the training happens MUCH slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = torch.randn(len(unique_users), embedding_dimension, requires_grad=True, device=device)\n",
    "F = torch.randn(len(unique_movies), embedding_dimension, requires_grad=True, device=device)\n",
    "optim = torch.optim.SGD([U, F], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(display):\n",
    "    M_hat = F @ U.T\n",
    "    difference = M - M_hat\n",
    "    loss_1 = (difference * M) ** 2\n",
    "    non_observation_mask = 1 - M\n",
    "    loss_2 = (difference * non_observation_mask) ** 2\n",
    "    loss = loss_1 + (0.005 * loss_2)\n",
    "    loss = loss.mean()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if display:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    display = True if i % 100 == 0 else False\n",
    "    training_loop(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, right? Let's dig into this a bit more.\n",
    "\n",
    "There are a few key differences between SGD and Adam. SGD on its own is effectively gradient descent at its most basic. It **only** takes the derivative of the loss with respect to each parameter and then descends the gradient to try and reach a minimum.\n",
    "\n",
    "Adam is SGD with a few improvements:\n",
    "\n",
    "1. Momentum: Adam essentially uses exponentially decaying gradients from previous updates to give the current weight update the context of previous updates. The consequence of this is that if previous updates tell us to adjust a weight in some way, and current updates tell us to adjust a weight in the same direction, the magnitude of the weight update will be increased. The key to this is the intuition of momentum -- previous motion in the weights has some impact on current motion.\n",
    "2. Adaptive Learning Rates: If certain parameters are only seen rarely in learning, Adam will scale up the gradient update.\n",
    "\n",
    "We can add momentum to our SGD algorithm. Lets see how much that gets us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([U, F], momentum=0.9, lr=0.01)\n",
    "\n",
    "for i in range(500):\n",
    "    display = True if i % 100 == 0 else False\n",
    "    training_loop(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a large improvement. However, there's still a **big** difference between Adam and SGD. And this makes sense. Our matrix is largely sparse, and embeddings (especially for users) are updated only a few times each epoch. Having adaptive learning rates which learn a lot for those users makes sense. Lets take away the momentum and just look at adaptive learning. \n",
    "\n",
    "We can do this using AdaGrad, which solely modifies SGD to magnify rare (or regularly small) parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adagrad([U, F], lr=0.01)\n",
    "\n",
    "for i in range(500):\n",
    "    display = True if i % 100 == 0 else False\n",
    "    training_loop(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us most of the way to Adam's performance! So, adaptive learning weights turn out to be *crucial* for learning in these sorts of factorization problems. Good to know!\n",
    "\n",
    "Thank you for following along in this tutorial with me. I hope its been some help. Happy optimizing ðŸ‘‹ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1636,
     "sourceId": 792972,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
